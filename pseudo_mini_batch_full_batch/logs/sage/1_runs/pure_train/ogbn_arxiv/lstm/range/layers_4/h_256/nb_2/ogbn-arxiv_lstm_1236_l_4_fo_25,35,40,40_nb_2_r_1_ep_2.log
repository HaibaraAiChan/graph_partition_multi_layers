Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648490211.043893
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0233154296875 GB
    Memory Allocated: 0.007684230804443359  GigaBytes
Max Memory Allocated: 0.007684230804443359  GigaBytes

The real block id is  3
get_global_graph_edges_ids_block function  spend 0.023807287216186523
range selection method range initialization spend 0.017310380935668945
time for parepare:  0.01802515983581543
local_output_nid generation:  0.010627985000610352
local_in_edges_tensor generation:  0.008716106414794922
mini_batch_src_global generation:  0.014046430587768555
r_  generation:  0.16960954666137695
local_output_nid generation:  0.011721372604370117
local_in_edges_tensor generation:  0.010636568069458008
mini_batch_src_global generation:  0.017083406448364258
r_  generation:  0.18273615837097168
----------------------check_connections_block total spend ----------------------------- 0.517554759979248
generate_one_block  0.19770359992980957
generate_one_block  0.20001816749572754
The real block id is  2
get_global_graph_edges_ids_block function  spend 0.03850531578063965
gen group dst list time:  0.007505178451538086
time for parepare:  0.019412517547607422
local_output_nid generation:  0.01737189292907715
local_in_edges_tensor generation:  0.04174017906188965
mini_batch_src_global generation:  0.04583239555358887
r_  generation:  0.5046782493591309
local_output_nid generation:  0.02541971206665039
local_in_edges_tensor generation:  0.04749917984008789
mini_batch_src_global generation:  0.057203054428100586
r_  generation:  0.5269856452941895
----------------------check_connections_block total spend ----------------------------- 1.5073251724243164
generate_one_block  0.6838228702545166
generate_one_block  0.6673128604888916
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.03803610801696777
gen group dst list time:  0.013969898223876953
time for parepare:  0.020396709442138672
local_output_nid generation:  0.022827863693237305
local_in_edges_tensor generation:  0.051442861557006836
mini_batch_src_global generation:  0.04962730407714844
r_  generation:  0.5529348850250244
local_output_nid generation:  0.031049489974975586
local_in_edges_tensor generation:  0.05006980895996094
mini_batch_src_global generation:  0.06129288673400879
r_  generation:  0.5788655281066895
----------------------check_connections_block total spend ----------------------------- 1.6575288772583008
generate_one_block  0.754298210144043
generate_one_block  0.7183959484100342
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.028455734252929688
gen group dst list time:  0.014508485794067383
time for parepare:  0.018942594528198242
local_output_nid generation:  0.024448871612548828
local_in_edges_tensor generation:  0.04444456100463867
mini_batch_src_global generation:  0.04937386512756348
r_  generation:  0.5299744606018066
local_output_nid generation:  0.03250718116760254
local_in_edges_tensor generation:  0.0467219352722168
mini_batch_src_global generation:  0.058725595474243164
r_  generation:  0.5439844131469727
----------------------check_connections_block total spend ----------------------------- 1.6218819618225098
generate_one_block  0.6546661853790283
generate_one_block  0.6464958190917969
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0233154296875 GB
    Memory Allocated: 0.007684230804443359  GigaBytes
Max Memory Allocated: 0.007684230804443359  GigaBytes

connection checking time:  4.786736011505127
block generation total time  4.1249918937683105
average batch blocks generation time:  2.0624959468841553
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2283935546875 GB
    Memory Allocated: 0.12742328643798828  GigaBytes
Max Memory Allocated: 0.12742328643798828  GigaBytes

torch.Size([167457, 128])
torch.Size([166712, 256])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 23.62 GiB total capacity; 21.96 GiB already allocated; 2.44 MiB free; 22.42 GiB reserved in total by PyTorch)
