Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648474220.03913
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.028063297271728516
range selection method range initialization spend 0.013698577880859375
time for parepare:  0.01790785789489746
local_output_nid generation:  0.005727529525756836
local_in_edges_tensor generation:  0.006412982940673828
mini_batch_src_global generation:  0.007512092590332031
r_  generation:  0.09177494049072266
local_output_nid generation:  0.006543397903442383
local_in_edges_tensor generation:  0.0031926631927490234
mini_batch_src_global generation:  0.009683847427368164
r_  generation:  0.10237789154052734
local_output_nid generation:  0.006616353988647461
local_in_edges_tensor generation:  0.0031147003173828125
mini_batch_src_global generation:  0.007637739181518555
r_  generation:  0.10176420211791992
local_output_nid generation:  0.006421327590942383
local_in_edges_tensor generation:  0.0032138824462890625
mini_batch_src_global generation:  0.00781869888305664
r_  generation:  0.10333132743835449
----------------------check_connections_block total spend ----------------------------- 0.5690798759460449
generate_one_block  0.11481428146362305
generate_one_block  0.11527848243713379
generate_one_block  0.11694550514221191
generate_one_block  0.11504530906677246
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.04653477668762207
gen group dst list time:  0.011957406997680664
time for parepare:  0.018927335739135742
local_output_nid generation:  0.014949560165405273
local_in_edges_tensor generation:  0.03674888610839844
mini_batch_src_global generation:  0.044849395751953125
r_  generation:  0.4710581302642822
local_output_nid generation:  0.022715330123901367
local_in_edges_tensor generation:  0.04329967498779297
mini_batch_src_global generation:  0.0568082332611084
r_  generation:  0.4766571521759033
local_output_nid generation:  0.02346515655517578
local_in_edges_tensor generation:  0.035610198974609375
mini_batch_src_global generation:  0.056366920471191406
r_  generation:  0.4832723140716553
local_output_nid generation:  0.023593664169311523
local_in_edges_tensor generation:  0.031125545501708984
mini_batch_src_global generation:  0.056833505630493164
r_  generation:  0.48936986923217773
----------------------check_connections_block total spend ----------------------------- 2.793825387954712
generate_one_block  0.6564455032348633
generate_one_block  0.6306099891662598
generate_one_block  0.6300559043884277
generate_one_block  0.6309695243835449
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.03278088569641113
gen group dst list time:  0.021384477615356445
time for parepare:  0.01865100860595703
local_output_nid generation:  0.023763179779052734
local_in_edges_tensor generation:  0.04345536231994629
mini_batch_src_global generation:  0.05662941932678223
r_  generation:  0.5569760799407959
local_output_nid generation:  0.033594369888305664
local_in_edges_tensor generation:  0.05500030517578125
mini_batch_src_global generation:  0.0653078556060791
r_  generation:  0.5632305145263672
local_output_nid generation:  0.03403139114379883
local_in_edges_tensor generation:  0.04616379737854004
mini_batch_src_global generation:  0.06398439407348633
r_  generation:  0.5731866359710693
local_output_nid generation:  0.03464221954345703
local_in_edges_tensor generation:  0.036391496658325195
mini_batch_src_global generation:  0.06418299674987793
r_  generation:  0.5714173316955566
----------------------check_connections_block total spend ----------------------------- 3.3331151008605957
generate_one_block  0.7347972393035889
generate_one_block  0.7411153316497803
generate_one_block  0.7319540977478027
generate_one_block  0.733614444732666
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

connection checking time:  6.126940488815308
block generation total time  5.489562034606934
average batch blocks generation time:  1.3723905086517334
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2030029296875 GB
    Memory Allocated: 0.10819530487060547  GigaBytes
Max Memory Allocated: 0.10819530487060547  GigaBytes

torch.Size([165792, 128])
torch.Size([157577, 128])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.62 GiB total capacity; 22.28 GiB already allocated; 12.44 MiB free; 22.41 GiB reserved in total by PyTorch)
