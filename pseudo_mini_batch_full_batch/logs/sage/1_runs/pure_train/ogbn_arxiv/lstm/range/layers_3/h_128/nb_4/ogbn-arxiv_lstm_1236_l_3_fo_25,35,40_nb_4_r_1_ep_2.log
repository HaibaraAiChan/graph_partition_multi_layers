Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648472116.036052
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.02395772933959961
range selection method range initialization spend 0.013707160949707031
time for parepare:  0.018153905868530273
local_output_nid generation:  0.005280971527099609
local_in_edges_tensor generation:  0.005101203918457031
mini_batch_src_global generation:  0.006498813629150391
r_  generation:  0.08275198936462402
local_output_nid generation:  0.006070375442504883
local_in_edges_tensor generation:  0.0030755996704101562
mini_batch_src_global generation:  0.00861668586730957
r_  generation:  0.08819746971130371
local_output_nid generation:  0.006205320358276367
local_in_edges_tensor generation:  0.003061532974243164
mini_batch_src_global generation:  0.006492137908935547
r_  generation:  0.08900070190429688
local_output_nid generation:  0.006072044372558594
local_in_edges_tensor generation:  0.003031015396118164
mini_batch_src_global generation:  0.006761074066162109
r_  generation:  0.09026551246643066
----------------------check_connections_block total spend ----------------------------- 0.505443811416626
generate_one_block  0.10093522071838379
generate_one_block  0.10179519653320312
generate_one_block  0.10264968872070312
generate_one_block  0.10186409950256348
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.034316062927246094
gen group dst list time:  0.011397123336791992
time for parepare:  0.01929616928100586
local_output_nid generation:  0.014542818069458008
local_in_edges_tensor generation:  0.03484964370727539
mini_batch_src_global generation:  0.03663372993469238
r_  generation:  0.39338064193725586
local_output_nid generation:  0.02222132682800293
local_in_edges_tensor generation:  0.036261796951293945
mini_batch_src_global generation:  0.04643678665161133
r_  generation:  0.39964818954467773
local_output_nid generation:  0.02360248565673828
local_in_edges_tensor generation:  0.030500411987304688
mini_batch_src_global generation:  0.052846431732177734
r_  generation:  0.41153740882873535
local_output_nid generation:  0.023832082748413086
local_in_edges_tensor generation:  0.02325439453125
mini_batch_src_global generation:  0.047020912170410156
r_  generation:  0.4095745086669922
----------------------check_connections_block total spend ----------------------------- 2.373211145401001
generate_one_block  0.546560525894165
generate_one_block  0.5231359004974365
generate_one_block  0.5302250385284424
generate_one_block  0.5265552997589111
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.030327558517456055
gen group dst list time:  0.021090984344482422
time for parepare:  0.018961668014526367
local_output_nid generation:  0.02330613136291504
local_in_edges_tensor generation:  0.03882908821105957
mini_batch_src_global generation:  0.04503488540649414
r_  generation:  0.48922181129455566
local_output_nid generation:  0.03371143341064453
local_in_edges_tensor generation:  0.04930520057678223
mini_batch_src_global generation:  0.05445575714111328
r_  generation:  0.4998507499694824
local_output_nid generation:  0.03506660461425781
local_in_edges_tensor generation:  0.03666186332702637
mini_batch_src_global generation:  0.05395150184631348
r_  generation:  0.5081822872161865
local_output_nid generation:  0.03844022750854492
local_in_edges_tensor generation:  0.03767752647399902
mini_batch_src_global generation:  0.05092215538024902
r_  generation:  0.5077385902404785
----------------------check_connections_block total spend ----------------------------- 2.9524013996124268
generate_one_block  0.6406433582305908
generate_one_block  0.6347594261169434
generate_one_block  0.6265697479248047
generate_one_block  0.6452090740203857
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

connection checking time:  5.325612545013428
block generation total time  4.67365837097168
average batch blocks generation time:  1.16841459274292
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2030029296875 GB
    Memory Allocated: 0.10294103622436523  GigaBytes
Max Memory Allocated: 0.10294103622436523  GigaBytes

torch.Size([164341, 128])
torch.Size([154783, 128])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 203, in forward
    x = self.layers[-1](blocks[-1], x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.62 GiB total capacity; 22.37 GiB already allocated; 2.44 MiB free; 22.42 GiB reserved in total by PyTorch)
