Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648473415.5121655
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.02717137336730957
range selection method range initialization spend 0.013443946838378906
time for parepare:  0.018146276473999023
local_output_nid generation:  0.005348920822143555
local_in_edges_tensor generation:  0.0063250064849853516
mini_batch_src_global generation:  0.0074918270111083984
r_  generation:  0.0910654067993164
local_output_nid generation:  0.0060312747955322266
local_in_edges_tensor generation:  0.0031058788299560547
mini_batch_src_global generation:  0.009702205657958984
r_  generation:  0.09589743614196777
local_output_nid generation:  0.006093502044677734
local_in_edges_tensor generation:  0.0031664371490478516
mini_batch_src_global generation:  0.0075528621673583984
r_  generation:  0.10092663764953613
local_output_nid generation:  0.006055116653442383
local_in_edges_tensor generation:  0.0031156539916992188
mini_batch_src_global generation:  0.008075237274169922
r_  generation:  0.10084247589111328
----------------------check_connections_block total spend ----------------------------- 0.5566654205322266
generate_one_block  0.11429643630981445
generate_one_block  0.11523127555847168
generate_one_block  0.11616802215576172
generate_one_block  0.11491060256958008
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.046474456787109375
gen group dst list time:  0.011998414993286133
time for parepare:  0.01941657066345215
local_output_nid generation:  0.01504969596862793
local_in_edges_tensor generation:  0.04096412658691406
mini_batch_src_global generation:  0.045439958572387695
r_  generation:  0.46605348587036133
local_output_nid generation:  0.025533437728881836
local_in_edges_tensor generation:  0.0411076545715332
mini_batch_src_global generation:  0.05561232566833496
r_  generation:  0.47547316551208496
local_output_nid generation:  0.027050018310546875
local_in_edges_tensor generation:  0.03451251983642578
mini_batch_src_global generation:  0.05540060997009277
r_  generation:  0.4847731590270996
local_output_nid generation:  0.02732110023498535
local_in_edges_tensor generation:  0.03489804267883301
mini_batch_src_global generation:  0.05724287033081055
r_  generation:  0.48869967460632324
----------------------check_connections_block total spend ----------------------------- 2.8004584312438965
generate_one_block  0.6554005146026611
generate_one_block  0.6259646415710449
generate_one_block  0.6348466873168945
generate_one_block  0.6341967582702637
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.03310751914978027
gen group dst list time:  0.021190166473388672
time for parepare:  0.019730091094970703
local_output_nid generation:  0.02435135841369629
local_in_edges_tensor generation:  0.036154985427856445
mini_batch_src_global generation:  0.04461383819580078
r_  generation:  0.5015239715576172
local_output_nid generation:  0.03709006309509277
local_in_edges_tensor generation:  0.03657984733581543
mini_batch_src_global generation:  0.05703306198120117
r_  generation:  0.49967145919799805
local_output_nid generation:  0.03883194923400879
local_in_edges_tensor generation:  0.04175257682800293
mini_batch_src_global generation:  0.05690503120422363
r_  generation:  0.5100317001342773
local_output_nid generation:  0.0385279655456543
local_in_edges_tensor generation:  0.047110557556152344
mini_batch_src_global generation:  0.05551624298095703
r_  generation:  0.5065884590148926
----------------------check_connections_block total spend ----------------------------- 2.996147632598877
generate_one_block  0.6298923492431641
generate_one_block  0.6449413299560547
generate_one_block  0.6363306045532227
generate_one_block  0.646766185760498
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

connection checking time:  5.796606063842773
block generation total time  5.108339071273804
average batch blocks generation time:  1.277084767818451
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2030029296875 GB
    Memory Allocated: 0.10593223571777344  GigaBytes
Max Memory Allocated: 0.10593223571777344  GigaBytes

torch.Size([165518, 128])
torch.Size([157634, 128])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.62 GiB total capacity; 22.40 GiB already allocated; 2.44 MiB free; 22.42 GiB reserved in total by PyTorch)
