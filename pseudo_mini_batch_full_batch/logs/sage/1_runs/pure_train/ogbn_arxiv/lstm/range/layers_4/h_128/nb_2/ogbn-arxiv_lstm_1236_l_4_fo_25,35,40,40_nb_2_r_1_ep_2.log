Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648489753.359223
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0194091796875 GB
    Memory Allocated: 0.002379894256591797  GigaBytes
Max Memory Allocated: 0.002379894256591797  GigaBytes

The real block id is  3
get_global_graph_edges_ids_block function  spend 0.02483057975769043
range selection method range initialization spend 0.013487815856933594
time for parepare:  0.018219709396362305
local_output_nid generation:  0.010518789291381836
local_in_edges_tensor generation:  0.00892186164855957
mini_batch_src_global generation:  0.014269351959228516
r_  generation:  0.17172861099243164
local_output_nid generation:  0.011849641799926758
local_in_edges_tensor generation:  0.010537147521972656
mini_batch_src_global generation:  0.01703786849975586
r_  generation:  0.18140220642089844
----------------------check_connections_block total spend ----------------------------- 0.5136940479278564
generate_one_block  0.20061087608337402
generate_one_block  0.20071005821228027
The real block id is  2
get_global_graph_edges_ids_block function  spend 0.04146575927734375
gen group dst list time:  0.007421970367431641
time for parepare:  0.01943039894104004
local_output_nid generation:  0.01645660400390625
local_in_edges_tensor generation:  0.03810262680053711
mini_batch_src_global generation:  0.047223806381225586
r_  generation:  0.5000321865081787
local_output_nid generation:  0.026129484176635742
local_in_edges_tensor generation:  0.04541802406311035
mini_batch_src_global generation:  0.0578923225402832
r_  generation:  0.5166316032409668
----------------------check_connections_block total spend ----------------------------- 1.4876232147216797
generate_one_block  0.6489377021789551
generate_one_block  0.6603660583496094
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.036049842834472656
gen group dst list time:  0.011747121810913086
time for parepare:  0.019526958465576172
local_output_nid generation:  0.022380828857421875
local_in_edges_tensor generation:  0.03525519371032715
mini_batch_src_global generation:  0.05370163917541504
r_  generation:  0.5451598167419434
local_output_nid generation:  0.03435039520263672
local_in_edges_tensor generation:  0.04940366744995117
mini_batch_src_global generation:  0.06223464012145996
r_  generation:  0.5574934482574463
----------------------check_connections_block total spend ----------------------------- 1.6249899864196777
generate_one_block  0.7404639720916748
generate_one_block  0.7142333984375
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.030583858489990234
gen group dst list time:  0.012324810028076172
time for parepare:  0.0194852352142334
local_output_nid generation:  0.023777008056640625
local_in_edges_tensor generation:  0.034787893295288086
mini_batch_src_global generation:  0.046785831451416016
r_  generation:  0.5159285068511963
local_output_nid generation:  0.03556013107299805
local_in_edges_tensor generation:  0.033129215240478516
mini_batch_src_global generation:  0.05935955047607422
r_  generation:  0.5217673778533936
----------------------check_connections_block total spend ----------------------------- 1.5255680084228516
generate_one_block  0.6455998420715332
generate_one_block  0.6550321578979492
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0194091796875 GB
    Memory Allocated: 0.002379894256591797  GigaBytes
Max Memory Allocated: 0.002379894256591797  GigaBytes

connection checking time:  4.638181209564209
block generation total time  4.064633131027222
average batch blocks generation time:  2.032316565513611
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2225341796875 GB
    Memory Allocated: 0.12213134765625  GigaBytes
Max Memory Allocated: 0.12213134765625  GigaBytes

torch.Size([167421, 128])
torch.Size([166642, 128])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.62 GiB total capacity; 22.35 GiB already allocated; 16.44 MiB free; 22.41 GiB reserved in total by PyTorch)
