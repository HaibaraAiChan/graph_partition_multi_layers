Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648487275.0511124
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0233154296875 GB
    Memory Allocated: 0.007684230804443359  GigaBytes
Max Memory Allocated: 0.007684230804443359  GigaBytes

The real block id is  3
get_global_graph_edges_ids_block function  spend 0.02330327033996582
random selection method range initialization spend 0.005738258361816406
time for parepare:  0.018860340118408203
local_output_nid generation:  0.015715360641479492
local_in_edges_tensor generation:  0.01387929916381836
mini_batch_src_global generation:  0.013967275619506836
r_  generation:  0.16932249069213867
local_output_nid generation:  0.021794557571411133
local_in_edges_tensor generation:  0.015154361724853516
mini_batch_src_global generation:  0.01721644401550293
r_  generation:  0.17829418182373047
----------------------check_connections_block total spend ----------------------------- 0.5419766902923584
generate_one_block  0.1959211826324463
generate_one_block  0.19828224182128906
The real block id is  2
get_global_graph_edges_ids_block function  spend 0.03819417953491211
gen group dst list time:  0.007548093795776367
time for parepare:  0.01919245719909668
local_output_nid generation:  0.024683475494384766
local_in_edges_tensor generation:  0.04769110679626465
mini_batch_src_global generation:  0.04900360107421875
r_  generation:  0.48929667472839355
local_output_nid generation:  0.031320810317993164
local_in_edges_tensor generation:  0.048425912857055664
mini_batch_src_global generation:  0.06357049942016602
r_  generation:  0.5025644302368164
----------------------check_connections_block total spend ----------------------------- 1.508223056793213
generate_one_block  0.6579141616821289
generate_one_block  0.6494424343109131
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.03195691108703613
gen group dst list time:  0.011785030364990234
time for parepare:  0.018959999084472656
local_output_nid generation:  0.03034806251525879
local_in_edges_tensor generation:  0.039684295654296875
mini_batch_src_global generation:  0.052710533142089844
r_  generation:  0.545952320098877
local_output_nid generation:  0.037558555603027344
local_in_edges_tensor generation:  0.053891897201538086
mini_batch_src_global generation:  0.06108808517456055
r_  generation:  0.5495815277099609
----------------------check_connections_block total spend ----------------------------- 1.6449158191680908
generate_one_block  0.728945255279541
generate_one_block  0.7106060981750488
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.025475502014160156
gen group dst list time:  0.012429952621459961
time for parepare:  0.019072771072387695
local_output_nid generation:  0.031126976013183594
local_in_edges_tensor generation:  0.039642333984375
mini_batch_src_global generation:  0.044841766357421875
r_  generation:  0.5116944313049316
local_output_nid generation:  0.03854656219482422
local_in_edges_tensor generation:  0.04380321502685547
mini_batch_src_global generation:  0.056848764419555664
r_  generation:  0.5151331424713135
----------------------check_connections_block total spend ----------------------------- 1.5436697006225586
generate_one_block  0.6317684650421143
generate_one_block  0.6521773338317871
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0233154296875 GB
    Memory Allocated: 0.007684230804443359  GigaBytes
Max Memory Allocated: 0.007684230804443359  GigaBytes

connection checking time:  4.696808576583862
block generation total time  4.030853748321533
average batch blocks generation time:  2.0154268741607666
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2283935546875 GB
    Memory Allocated: 0.12743759155273438  GigaBytes
Max Memory Allocated: 0.12743759155273438  GigaBytes

torch.Size([167445, 128])
torch.Size([166713, 256])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 23.62 GiB total capacity; 21.93 GiB already allocated; 2.44 MiB free; 22.42 GiB reserved in total by PyTorch)
