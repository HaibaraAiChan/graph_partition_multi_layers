Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648459951.023108
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.025001049041748047
random selection method range initialization spend 0.00649261474609375
time for parepare:  0.018223285675048828
local_output_nid generation:  0.00830078125
local_in_edges_tensor generation:  0.009005546569824219
mini_batch_src_global generation:  0.006479501724243164
r_  generation:  0.0822453498840332
local_output_nid generation:  0.011543989181518555
local_in_edges_tensor generation:  0.005964517593383789
mini_batch_src_global generation:  0.00864267349243164
r_  generation:  0.0881047248840332
local_output_nid generation:  0.011772632598876953
local_in_edges_tensor generation:  0.00603032112121582
mini_batch_src_global generation:  0.0063457489013671875
r_  generation:  0.08869409561157227
local_output_nid generation:  0.011876344680786133
local_in_edges_tensor generation:  0.006041049957275391
mini_batch_src_global generation:  0.007195711135864258
r_  generation:  0.09138751029968262
----------------------check_connections_block total spend ----------------------------- 0.5488882064819336
generate_one_block  0.10187053680419922
generate_one_block  0.1027989387512207
generate_one_block  0.10084915161132812
generate_one_block  0.10177922248840332
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.03912067413330078
gen group dst list time:  0.011228084564208984
time for parepare:  0.019466638565063477
local_output_nid generation:  0.019170284271240234
local_in_edges_tensor generation:  0.04247879981994629
mini_batch_src_global generation:  0.040752410888671875
r_  generation:  0.3919868469238281
local_output_nid generation:  0.02875494956970215
local_in_edges_tensor generation:  0.03812098503112793
mini_batch_src_global generation:  0.04698324203491211
r_  generation:  0.40594053268432617
local_output_nid generation:  0.029103994369506836
local_in_edges_tensor generation:  0.03130507469177246
mini_batch_src_global generation:  0.04735374450683594
r_  generation:  0.4097280502319336
local_output_nid generation:  0.029337167739868164
local_in_edges_tensor generation:  0.02474832534790039
mini_batch_src_global generation:  0.04743695259094238
r_  generation:  0.4076497554779053
----------------------check_connections_block total spend ----------------------------- 2.4190897941589355
generate_one_block  0.5526213645935059
generate_one_block  0.5311141014099121
generate_one_block  0.5345802307128906
generate_one_block  0.5318686962127686
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.03388857841491699
gen group dst list time:  0.020756959915161133
time for parepare:  0.01952672004699707
local_output_nid generation:  0.03066086769104004
local_in_edges_tensor generation:  0.046602487564086914
mini_batch_src_global generation:  0.04535651206970215
r_  generation:  0.4920969009399414
local_output_nid generation:  0.03648233413696289
local_in_edges_tensor generation:  0.05496501922607422
mini_batch_src_global generation:  0.0553584098815918
r_  generation:  0.4945652484893799
local_output_nid generation:  0.036518096923828125
local_in_edges_tensor generation:  0.042874813079833984
mini_batch_src_global generation:  0.05469655990600586
r_  generation:  0.5104062557220459
local_output_nid generation:  0.03669428825378418
local_in_edges_tensor generation:  0.04277610778808594
mini_batch_src_global generation:  0.05496788024902344
r_  generation:  0.5127232074737549
----------------------check_connections_block total spend ----------------------------- 3.01621150970459
generate_one_block  0.6397392749786377
generate_one_block  0.6385278701782227
generate_one_block  0.6246471405029297
generate_one_block  0.6516304016113281
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

connection checking time:  5.435301303863525
block generation total time  4.704729080200195
average batch blocks generation time:  1.1761822700500488
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2030029296875 GB
    Memory Allocated: 0.10291147232055664  GigaBytes
Max Memory Allocated: 0.10291147232055664  GigaBytes

torch.Size([164291, 128])
torch.Size([154638, 128])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 203, in forward
    x = self.layers[-1](blocks[-1], x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.62 GiB total capacity; 22.37 GiB already allocated; 2.44 MiB free; 22.42 GiB reserved in total by PyTorch)
