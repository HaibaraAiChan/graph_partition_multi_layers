Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648462093.8840113
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.02630472183227539
random selection method range initialization spend 0.005702972412109375
time for parepare:  0.019028186798095703
local_output_nid generation:  0.00835108757019043
local_in_edges_tensor generation:  0.008208036422729492
mini_batch_src_global generation:  0.0077402591705322266
r_  generation:  0.09261322021484375
local_output_nid generation:  0.011667013168334961
local_in_edges_tensor generation:  0.005961179733276367
mini_batch_src_global generation:  0.010223865509033203
r_  generation:  0.0980217456817627
local_output_nid generation:  0.011813640594482422
local_in_edges_tensor generation:  0.006014823913574219
mini_batch_src_global generation:  0.007689476013183594
r_  generation:  0.09888529777526855
local_output_nid generation:  0.011846303939819336
local_in_edges_tensor generation:  0.006095409393310547
mini_batch_src_global generation:  0.008791685104370117
r_  generation:  0.10638809204101562
----------------------check_connections_block total spend ----------------------------- 0.6113109588623047
generate_one_block  0.11565685272216797
generate_one_block  0.11509442329406738
generate_one_block  0.11417889595031738
generate_one_block  0.11595940589904785
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.04099416732788086
gen group dst list time:  0.01215505599975586
time for parepare:  0.01911163330078125
local_output_nid generation:  0.020082950592041016
local_in_edges_tensor generation:  0.04519987106323242
mini_batch_src_global generation:  0.046704769134521484
r_  generation:  0.46796751022338867
local_output_nid generation:  0.026144981384277344
local_in_edges_tensor generation:  0.04479050636291504
mini_batch_src_global generation:  0.0579073429107666
r_  generation:  0.47733569145202637
local_output_nid generation:  0.02589702606201172
local_in_edges_tensor generation:  0.03743696212768555
mini_batch_src_global generation:  0.05990409851074219
r_  generation:  0.48138856887817383
local_output_nid generation:  0.026355504989624023
local_in_edges_tensor generation:  0.04061722755432129
mini_batch_src_global generation:  0.05780935287475586
r_  generation:  0.48690080642700195
----------------------check_connections_block total spend ----------------------------- 2.8473153114318848
generate_one_block  0.6607110500335693
generate_one_block  0.6289339065551758
generate_one_block  0.621375560760498
generate_one_block  0.6290452480316162
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.03384518623352051
gen group dst list time:  0.02148580551147461
time for parepare:  0.01870250701904297
local_output_nid generation:  0.03074336051940918
local_in_edges_tensor generation:  0.0504155158996582
mini_batch_src_global generation:  0.05729866027832031
r_  generation:  0.5600874423980713
local_output_nid generation:  0.03763985633850098
local_in_edges_tensor generation:  0.05692887306213379
mini_batch_src_global generation:  0.06681370735168457
r_  generation:  0.5713386535644531
local_output_nid generation:  0.03789496421813965
local_in_edges_tensor generation:  0.04235219955444336
mini_batch_src_global generation:  0.07027363777160645
r_  generation:  0.5693833827972412
local_output_nid generation:  0.0373687744140625
local_in_edges_tensor generation:  0.04721951484680176
mini_batch_src_global generation:  0.06522250175476074
r_  generation:  0.5721392631530762
----------------------check_connections_block total spend ----------------------------- 3.4013748168945312
generate_one_block  0.738572359085083
generate_one_block  0.7243812084197998
generate_one_block  0.7248406410217285
generate_one_block  0.7334942817687988
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0174560546875 GB
    Memory Allocated: 0.0017633438110351562  GigaBytes
Max Memory Allocated: 0.0017633438110351562  GigaBytes

connection checking time:  6.248690128326416
block generation total time  5.4613542556762695
average batch blocks generation time:  1.3653385639190674
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2030029296875 GB
    Memory Allocated: 0.1081705093383789  GigaBytes
Max Memory Allocated: 0.1081705093383789  GigaBytes

torch.Size([165699, 128])
torch.Size([157497, 128])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.62 GiB total capacity; 22.26 GiB already allocated; 2.44 MiB free; 22.42 GiB reserved in total by PyTorch)
