Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648464167.9575546
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0213623046875 GB
    Memory Allocated: 0.005230903625488281  GigaBytes
Max Memory Allocated: 0.005230903625488281  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.027504920959472656
random selection method range initialization spend 0.006310224533081055
time for parepare:  0.018602371215820312
local_output_nid generation:  0.008379459381103516
local_in_edges_tensor generation:  0.00942087173461914
mini_batch_src_global generation:  0.007363319396972656
r_  generation:  0.0896449089050293
local_output_nid generation:  0.011817216873168945
local_in_edges_tensor generation:  0.0061609745025634766
mini_batch_src_global generation:  0.010002851486206055
r_  generation:  0.09658575057983398
local_output_nid generation:  0.011920452117919922
local_in_edges_tensor generation:  0.006088972091674805
mini_batch_src_global generation:  0.007732391357421875
r_  generation:  0.10128927230834961
local_output_nid generation:  0.011989831924438477
local_in_edges_tensor generation:  0.006154298782348633
mini_batch_src_global generation:  0.008577346801757812
r_  generation:  0.10506868362426758
----------------------check_connections_block total spend ----------------------------- 0.6071879863739014
generate_one_block  0.11632132530212402
generate_one_block  0.11688423156738281
generate_one_block  0.11841201782226562
generate_one_block  0.11639523506164551
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.044434309005737305
gen group dst list time:  0.011986494064331055
time for parepare:  0.019008159637451172
local_output_nid generation:  0.020906686782836914
local_in_edges_tensor generation:  0.043721675872802734
mini_batch_src_global generation:  0.04987668991088867
r_  generation:  0.4720571041107178
local_output_nid generation:  0.026647090911865234
local_in_edges_tensor generation:  0.05020785331726074
mini_batch_src_global generation:  0.05630326271057129
r_  generation:  0.4964454174041748
local_output_nid generation:  0.028336524963378906
local_in_edges_tensor generation:  0.05986356735229492
mini_batch_src_global generation:  0.08098649978637695
r_  generation:  0.4839935302734375
local_output_nid generation:  0.027507543563842773
local_in_edges_tensor generation:  0.03671073913574219
mini_batch_src_global generation:  0.06083083152770996
r_  generation:  0.4879262447357178
----------------------check_connections_block total spend ----------------------------- 2.9441916942596436
generate_one_block  0.6824080944061279
generate_one_block  0.6459646224975586
generate_one_block  0.6481597423553467
generate_one_block  0.6431446075439453
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.05194664001464844
gen group dst list time:  0.0243074893951416
time for parepare:  0.01904010772705078
local_output_nid generation:  0.03115558624267578
local_in_edges_tensor generation:  0.05358767509460449
mini_batch_src_global generation:  0.04630255699157715
r_  generation:  0.4951300621032715
local_output_nid generation:  0.038048744201660156
local_in_edges_tensor generation:  0.05839204788208008
mini_batch_src_global generation:  0.05770993232727051
r_  generation:  0.5030355453491211
local_output_nid generation:  0.03729534149169922
local_in_edges_tensor generation:  0.04056668281555176
mini_batch_src_global generation:  0.06044936180114746
r_  generation:  0.503800630569458
local_output_nid generation:  0.0376434326171875
local_in_edges_tensor generation:  0.04139351844787598
mini_batch_src_global generation:  0.05852079391479492
r_  generation:  0.5165255069732666
----------------------check_connections_block total spend ----------------------------- 3.060180187225342
generate_one_block  0.6595766544342041
generate_one_block  0.6707253456115723
generate_one_block  0.6577303409576416
generate_one_block  0.6861221790313721
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0213623046875 GB
    Memory Allocated: 0.005230903625488281  GigaBytes
Max Memory Allocated: 0.005230903625488281  GigaBytes

connection checking time:  6.004371881484985
block generation total time  5.2938315868377686
average batch blocks generation time:  1.3234578967094421
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2069091796875 GB
    Memory Allocated: 0.1093301773071289  GigaBytes
Max Memory Allocated: 0.1093301773071289  GigaBytes

torch.Size([165404, 128])
torch.Size([157360, 256])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 23.62 GiB total capacity; 21.95 GiB already allocated; 14.44 MiB free; 22.41 GiB reserved in total by PyTorch)
