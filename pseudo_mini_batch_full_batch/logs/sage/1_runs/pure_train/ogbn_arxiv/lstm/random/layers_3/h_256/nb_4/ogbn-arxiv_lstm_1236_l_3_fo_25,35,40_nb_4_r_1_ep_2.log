Using backend: pytorch
WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.3.
main start at this time 1648462989.549949
-----------------------------------------before load data 
 Nvidia-smi: 0.1717529296875 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

ogbn-arxiv
# Nodes: 169343
# Edges: 2315598
# Train: 90941
# Val: 29799
# Test: 48603
# Classes: 40

in feats:  128
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 1.0213623046875 GB
    Memory Allocated: 0.005230903625488281  GigaBytes
Max Memory Allocated: 0.005230903625488281  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.02512216567993164
random selection method range initialization spend 0.007245779037475586
time for parepare:  0.019766569137573242
local_output_nid generation:  0.008433818817138672
local_in_edges_tensor generation:  0.009436845779418945
mini_batch_src_global generation:  0.007842540740966797
r_  generation:  0.08577394485473633
local_output_nid generation:  0.01178288459777832
local_in_edges_tensor generation:  0.007723093032836914
mini_batch_src_global generation:  0.009281635284423828
r_  generation:  0.08897972106933594
local_output_nid generation:  0.011982440948486328
local_in_edges_tensor generation:  0.007885932922363281
mini_batch_src_global generation:  0.006600379943847656
r_  generation:  0.0899350643157959
local_output_nid generation:  0.011957645416259766
local_in_edges_tensor generation:  0.008068323135375977
mini_batch_src_global generation:  0.0074231624603271484
r_  generation:  0.09225296974182129
----------------------check_connections_block total spend ----------------------------- 0.5697691440582275
generate_one_block  0.10349774360656738
generate_one_block  0.1036520004272461
generate_one_block  0.1038503646850586
generate_one_block  0.10390877723693848
The real block id is  1
get_global_graph_edges_ids_block function  spend 0.036165475845336914
gen group dst list time:  0.011521577835083008
time for parepare:  0.019385576248168945
local_output_nid generation:  0.01962757110595703
local_in_edges_tensor generation:  0.03981828689575195
mini_batch_src_global generation:  0.037213802337646484
r_  generation:  0.39872050285339355
local_output_nid generation:  0.02584075927734375
local_in_edges_tensor generation:  0.0386199951171875
mini_batch_src_global generation:  0.0468297004699707
r_  generation:  0.41072535514831543
local_output_nid generation:  0.026045560836791992
local_in_edges_tensor generation:  0.03220725059509277
mini_batch_src_global generation:  0.05012702941894531
r_  generation:  0.4189443588256836
local_output_nid generation:  0.02611541748046875
local_in_edges_tensor generation:  0.031745195388793945
mini_batch_src_global generation:  0.04785919189453125
r_  generation:  0.4210522174835205
----------------------check_connections_block total spend ----------------------------- 2.4553024768829346
generate_one_block  0.5556199550628662
generate_one_block  0.538978099822998
generate_one_block  0.5515673160552979
generate_one_block  0.5408995151519775
The real block id is  0
get_global_graph_edges_ids_block function  spend 0.041028738021850586
gen group dst list time:  0.024361610412597656
time for parepare:  0.01906275749206543
local_output_nid generation:  0.03166675567626953
local_in_edges_tensor generation:  0.06232452392578125
mini_batch_src_global generation:  0.045049428939819336
r_  generation:  0.49999141693115234
local_output_nid generation:  0.037909746170043945
local_in_edges_tensor generation:  0.05213761329650879
mini_batch_src_global generation:  0.055524349212646484
r_  generation:  0.5053286552429199
local_output_nid generation:  0.037680864334106445
local_in_edges_tensor generation:  0.04395127296447754
mini_batch_src_global generation:  0.05187344551086426
r_  generation:  0.5211601257324219
local_output_nid generation:  0.03777766227722168
local_in_edges_tensor generation:  0.04382038116455078
mini_batch_src_global generation:  0.0536656379699707
r_  generation:  0.5285632610321045
----------------------check_connections_block total spend ----------------------------- 3.0758488178253174
generate_one_block  0.6450250148773193
generate_one_block  0.6335737705230713
generate_one_block  0.6400618553161621
generate_one_block  0.6440157890319824
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 1.0213623046875 GB
    Memory Allocated: 0.005230903625488281  GigaBytes
Max Memory Allocated: 0.005230903625488281  GigaBytes

connection checking time:  5.531151294708252
block generation total time  4.749741315841675
average batch blocks generation time:  1.1874353289604187
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 1.2069091796875 GB
    Memory Allocated: 0.10632467269897461  GigaBytes
Max Memory Allocated: 0.10632467269897461  GigaBytes

torch.Size([164189, 128])
torch.Size([154577, 256])
Traceback (most recent call last):
  File "pseudo_mini_batch_range_arxiv_sage.py", line 435, in <module>
    main()
  File "pseudo_mini_batch_range_arxiv_sage.py", line 431, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_arxiv_sage.py", line 251, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_arxiv.py", line 182, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.62 GiB total capacity; 21.97 GiB already allocated; 16.44 MiB free; 22.41 GiB reserved in total by PyTorch)
