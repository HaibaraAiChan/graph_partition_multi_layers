ogbn-arxiv sage,"pseudo 2 batches 25,35,40,40","pseudo 4 batches 25,35,40,40","pseudo 8 batches 25,35,40,40","pseudo 16 batches 25,35,40,40","pseudo 32 batches 25,35,40,40","pseudo 64 batches 25,35,40,40"
Nvidia-smi,OOM,OOM,OOM,23.5897216796875,21.8065185546875,19.1072998046875
CUDA_mem,OOM,OOM,OOM,20.330527782440186,18.49724817276001,16.527770042419434
CUDA_max_mem,OOM,OOM,OOM,20.46664810180664,18.667250633239746,16.79509925842285
epoch_time,,,,74.9022216796875,131.4373688697815,230.58962273597717
pure train_time per epoch,,,,15.631847143173218,27.692137241363525,51.50266623497009
connect checking time per epoch: ,,,,29.41250705718994,52.35801911354065,89.58828616142273
block generation time per epoch: ,,,,25.952467441558838,45.732635736465454,80.4489221572876
batches generation time per epoch: ,,,,1.6220292150974274,1.4291448667645454,1.2570144087076187
first layer input nodes number per epoch,,,,2617925.0,5146647.0,10011655.0
first layer num_input nodes * in_feats per epoch,,,,335094400.0,658770816.0,1281491840.0
logged input_features_size transfer (pointers* Bytes),,,,2304.0,4608.0,9216.0
logged block_size_to_device transfer (pointers*  Bytes),,,,1536.0,3072.0,6144.0
load block tensor time per epoch,,,,0.42716360092163086,1.2190518379211426,2.4841151237487793
block to device time per epoch,,,,0.42937493324279785,0.8379185199737549,1.4991850852966309
